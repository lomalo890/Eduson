Многозадачность -- возможность выполнять несколько задачи параллельно. Ускорение и эффективное пользование железом.
Последовательное выполнение
РАспараллеливание нужно для усорения работы программистов.
Вытесняющая многозадачность -- это способ выполнения нескольких задач путём быстрого переключения между ними.

состояние гонки (race condition) или неопределённость параллелизма -- ситуация, когда результат зависит от порядка, в котором выполняются программы или подзадачи.
Чтобы избежать состояние гонки, нужно обеспечить потоковую безопасность
Потокобезопасные операции - операции, которые дают всё время одинаковый результат, независимо от того, выполняютсяя они параллелльно или нет.
Непотокобезопасные -- операци, результат которых может меняться в зависимости от порядка выполнения шагов внутри задач.

Старые ЯП не предоставляют инструментов для потоковой безопасности. В Python есть -- это примитивы синхронизации (mutex (замок), семафор)
mutex запрещает параллельный доступ к объекту, то есть читать и записывать данные может только одна операция. Другая операция не может начаться.
Deadlock -- состояние, когда потоки не могут закончить работу из-за того, что нужные ресурсы заблокированы другими потоками
Акторы -- две программы. Одна программа читает чётные числа, другая нечётные

Есть ещё семафор, то есть к объекту в одном потоке может обращаться несколько потоков
Есть Erlang и Elixir для акторов

global interpreter lock --запрещает нескольким потокам работать одновременно. Многозадачность достигается через постоянное переключение между разными потоками. Внутри каждого потока выделяются атомарные операции. Переключение во времея этих операций невозможно  Когда операция завершилась, происходит переключение на другой поток.

Асинхронное програмирование заключает в себе возможность для программиста, когда переключение между корутинами (подпрограммами) может происходить, а когда не может.
Кооперативная многозадачность -- вид многозадачности, при котором выполнение не прерывается, а передаётся другим программам.
В некоторых ОС есть кооперативная многозадачность, то есть выполнение одной операции может передаваться другой программе. Такие ОС называют файберами.
Есть CPU-bound task - зависит от процессора.
Есть I/O-bound task - зависит от устройств ввода-вывода
I/O-bound task зависит от запроса и, в большинстве своём, из ожидания. (запрашиваем у сети видео)
Он также может быть блокирующим и нет.
А для ускорения CPU-bound задач можно добавлять ядра процессора в работу.
В I/O-bound task для ускорения работы надо избегать блокировок других задач. То есть сетевая карта при заполнении от скачивания файла, говорит компу Эзабери эти данные, а я продолжу дальше скачивать".
Распределённые вычисления -- это способ запустить программу на нескольких устройствах одновременной. То есть 100 компов соеденины с сервером. Они дают серверу свои выисления и сервер выдаёт финальный результат.
Publish-Subscribe -- тип взимодействия устройств, в котором устройства могут меняться данными друг с другом.
То есть они связаны общей почтой -- менеджером очередей (RabbitMQ, Kafka)


Для распараллеливания есть ThreadPooExecutor из модуля concurrent.future

integers = [1,6,3,8,5,2,6,9,54,2,6,9,5,36,2]

start = time.monotonic() # Сохраняем время начала программы
executor = ThreadPoolExecutor() # Создаём с максимальным количеством потоков по умолчанию
for integer in integers:
    executor.submit(squire, integer) # Приводит все элементы списка в квадрат
executor.shutdown # Он нам нужен теперь? Нет.

Вот так программа может выполниться за 4 секунды, а мы ускорили её на две.

submit(squire, integer) == map(squire, integer)

Также мы можем с помощью with создать и закрыть потоки

with ThreadPoolExecutor() as executor:
    executor.map(squire, integer)

Моожно поставить на squire замок, чтобы он выполнялся в одном потоке, а не в нескольких сразу же.
Чтобы показать пример, нужно глобализовать переменную integer, чтобы каждый поток мог работать с ней. То есть чтобы она была одна на всех.

lock = Lock()

def squire():
    global integer
    lock.acquir()
    local = integer # локализуем переменную, чтобы работать с ней в функции
    sleep(0) # В качестве промерки. Поток всё равно знает, что надо заснуть на время, пока другие программы будут работать. Тогда возникнет состояние гонки, а нам этого не надо.
    local = local ** 2
    integer = local
    lock.release()

Каждый раз создавать лок и следить за тем, что он у нас где-то открывается, а где-то закрывается -- следить замучиешься. Не лучше ли использовать очередь?

q = Queue() # Автоматически работает в потоках. Не нужно её туда пихать.

def plus_one():
    global q
    local = q.get(block=True) # аргумент говорит о том программе, что не надо истерить, если очередь пуста. Просто оповести об этом.
    sleep(0)
    local += 1
    q.put(local)

with ThreadPoolExecutor() as executor:
    executor.map(executor.submit(plus_one), for _ in range(100))

У ThreadPoolExecutor может быть аргумент max_worker, где мы задаём количество потоков.





Вот есть список из 8-х элементов и есть процессор на 4 ядра. Если тред использует потоки в ядрах, то ProcessPoolExecutor говорит, сколько ядер задействовать. ОС само решит сколько потоков нужно.

integers = [2, 3, 4, 5]

def plus_one(integer):
    return integer + 1

start = monotonic()
integers = map(plus_one, integers)
diff = monotonic() - start

Выполнится за 1 секунду. Попробуем то же самое сделать для одного элемента:

start = monotonic()
integers = plus_one(5)
diff = monotonic() - start

За 0.25 секунды. Нам нужно, чтобы эта операция выполнялась быстрее.

start = monotonic()

with ProcessPoolExecutor(max_worker=4) as executor:
    integers = executor.map(plus_one, integers)
diff = monotonic() - start

Теперь выполнится за 0.40. А что так долго? Да потому что на разделение задачи на ядра (4) и на копирование в каждый поток тоже требуется время.
Процесс на спавне в винде и маке
А в юникс форки
В отличии от потоков, которые работают в одном интерпретаторе и взаимодействуют с одной памятью, процессы же запускают каждый свой интерпретатор и каждый взаимодействует со своим пространством памяти, что, в отличии от потоков, деалет сложным нахождение ошибки.

Предложение к тебе, мой програмист:
Давай запускать в процессы те части кода, в которых мы уверены.















Асинхронность


async def plus_one():
    x = await 1 + 1
    y = await 2 + 2
    z = await 3 + 3
    return x + y + z

Асинхроннаяя функция, то есть предназначена для выполнения в потоках, в данным случае трёх. То есть мы говорим: один из трёх потоков может складывать 1 + 1, а я пока дальше пойду.


По другому:

async def foo():
    asyncio.gather(1 + 1, 2 + 2, 3 + 3)

Можно успыпить поток: asyncio.sleep(.3)



Напишем пример:

async def one():
    await asyncio.sleep(.1) #await означает: никтоне хочет выполнится на это время?
    return 1

async def two():
    await asyncio.sleep(.2)
    return 2

async def three():
    await asyncio.sleep(.3)
    return 3

async all():
    tasks = [three(), one(), two()] # заметь, как мы расставили
    for task in asyncio.as_completed(tasks):
        result = await task # сначала one, потому что первым выполнится, и так далее. Это всё из-за as_completed
        print(result)


Чтобы распаралелить эти функции:
asyncio.gather(one(). two(), three())










